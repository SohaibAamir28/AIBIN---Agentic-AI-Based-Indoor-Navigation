# AIBIN---Agentic-AI-Based-Indoor-Navigation
AIBIN: Agentic AI Based Indoor Navigation 

# Agentic AI-Based Indoor Navigation

## Project Overview

**Agentic AI-Based Indoor Navigation** uses advanced AI and augmented reality (AR) to deliver personalized, real-time navigation experiences. The system integrates **object recognition** via **CV techniques**, **AI decision-making** through **LangChain** and **LangGraph**, and a **knowledge graph** powered by **Neo4j**. The AI adapts to user preferences, offering tailored, interactive navigation guidance for indoor spaces.

## Hackathon Details

- **Hackathon Start**: Friday, 8:00 PM PKT  
- **Hackathon End**: Sunday, 11:59 PM PKT  
- **Team Registration Deadline**: Saturday Night PKT

### **Project Category**
Indoor Navigation

### **Project Description**
This project integrates **AI** with **AR** to enhance **indoor navigation** by recognizing objects, providing context-aware navigation, and interacting with users via AI. The **LangChain** framework enables AI-driven interactions, while **LangGraph** orchestrates dynamic decision-making. The system uses **Neo4j** for managing data related to indoor objects and exhibits, providing personalized user guidance.

---

## **Roadmap for Development**

### **Phase 1: Discovery and Planning (0-2 hours)**

- **Tasks**:
  1. Finalize project **title** and **category**.
  2. Confirm **core features**: AI-based navigation, object recognition, voice interaction.
  3. Set up **project management tools**: GitHub, Slack, Trello.
  4. Define roles and assign tasks to team members.

- **Deliverables**:
  - Clear **project scope** and **team roles**.

---

### **Phase 2: Backend & Core Development (2-4 hours)**

- **Tasks**:
  1. Set up **FastAPI** backend server.
  2. Install **LangChain**, **Neo4j**, **TensorFlow**, **OpenCV** for object detection.
  3. **Object Recognition**: Implement object detection using **OpenCV** or **YOLO**.
  4. **OCR**: Integrate **Tesseract** or **Google Vision API** for text extraction from objects.
  5. Set up **Neo4j** to store object data and relationships.

- **Deliverables**:
  - **Backend API** for object recognition and AI responses.
  - **Object detection** and **OCR functionality** working.

---

### **Phase 3: AI Integration (4-6 hours)**

- **Tasks**:
  1. **Integrate LangChain** for AI-based conversations and responses.
  2. Use **LangGraph** to model dynamic decision-making for context-aware navigation.
  3. **Connect Neo4j** with the AI system to query objects and provide real-time personalized navigation information.

- **Deliverables**:
  - **AI-powered object recognition** and **real-time responses** to user queries.

---

### **Phase 4: UI/UX & Final Adjustments (6-8 hours)**

- **Tasks**:
  1. **Design a simple UI** for user interaction (e.g., navigation commands, object query buttons).
  2. Optimize **real-time AI responses** and **navigation feedback** for smooth user experience.
  3. Ensure **cross-device compatibility** and test on **Android** for performance.

- **Deliverables**:
  - **UI/UX for user interaction** with AI.
  - **Optimized performance** for smooth interaction.

---

### **Phase 5: Final Submission (8-12 hours)**

- **Tasks**:
  1. **Prepare Demo Video**: Showcase key features (object recognition, AI interaction, navigation).
  2. **Submit** project code and demo video to the hackathon platform.

- **Deliverables**:
  - **Demo Video** submitted.
  - **Codebase** uploaded and final **project details** filled.

---

## **Technologies Used**

- **Backend**: FastAPI, LangChain, Neo4j
- **AI**: LangGraph, Llama 3 (for AI reasoning and decision-making)
- **AR**: ARCore, Unreal Engine
- **OCR**: Tesseract / Google Vision API
- **Database**: Neo4j (for knowledge graph)

---

## **Important Links**

- **Hackathon Registration**: [Pak Angels Gen AI Training - Cohort 6](https://hackathon.aspirepk.org/cohort6/)
- **Hackathon Details**: [Event Overview](https://hackathon.aspirepk.org/cohort6/)

---

### **Team Members**

- **Team Leader**: Sohaib Aamir  
  - **Team Member**: Ahsan Abdullah

### **Resources**

- **LangChain Documentation**: [Link](https://python.langchain.com/)
- **Neo4j Setup**: [Link](https://neo4j.com/docs/)
- **ARCore SDK**: [Link](https://developers.google.com/ar/develop)
- **Tesseract OCR**: [Link](https://github.com/tesseract-ocr/tesseract)
- **YOLO Object Detection**: [Link](https://github.com/AlexeyAB/darknet)

---

This **README.md** format provides a clear, concise roadmap for your project, ensuring your team knows what to do within the limited time frame. Feel free to copy, paste, and customize it further!
